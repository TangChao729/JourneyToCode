# Online Coding Course Experience

This repository contains my reflections and learnings from an online coding course that I recently completed. The course was taught by [DeepLearning.AI](https://www.deeplearning.ai/) in collaboration with [Coursera](https://www.coursera.org/).

## Course Overview

Large Language Models have gained a lot of attention recently. They are used in many applications such as machine translation, speech recognition, etc. The debut of ChatGPT has fundamentally changed the way we use language models. As a result, many people are interested in learning how to build their own language models. DeepLearning.AI has released a series of courses that teach you how to build your own language models, from the basic to the state-of-the-art. The courses I took mainly focused on catching up with the recent up-rise of language models:

1. [Deep Learning Specialization](https://www.deeplearning.ai/courses/machine-learning-specialization/)
2. [Generative AI with LLMs](https://www.deeplearning.ai/courses/generative-ai-with-llms/)
3. [MLops Specialization](https://www.deeplearning.ai/courses/machine-learning-engineering-for-production-mlops/)
4. [Finetuning Large Language Models](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)

## My Experience

From the moment I used ChatGPT, I knew that this will be my future direction. I was so excited that current technology and engineering can achieve such a high level of performance. But this is a relatively new field, and the course I took at the University of Melbourne, the Natural Language Processing, although was taught by a very good professor [Dr. Jey Han Lau](https://findanexpert.unimelb.edu.au/profile/385601-jey-han-lau), was not enough to prepare me for this field. 

Hence, I decided to take this series of courses. I was not disappointed. All of them are very well designed and taught. The Deep Learning Specialization mainly focus on preping you with the basic knowledge of deep learning, such as various neural network architectures, from simple classification to convolutional neural networks, to recurrent neural networks. This acts as a warp-up of my university learning for me.

The Generative AI with LLMs course is the main course that I took. It teaches you how modern language models work, from the basic to the state-of-the-art. The course summarizes the recent cutting-edge development in this field, and the important components, other than the model itself, that are needed to build a language model, such as Prompt Engineering, Retrieval Augmented Generation (RAG), Parameter efficient fine-tuning (PERF), etc. All these are very important to build a language model due to the large amount of data and computation power needed to train a language model.

The MLops Specialization is a course that teaches you how a machine learning engineer works. It brings you through the whole process of building a machine learning model, from data collection, data cleaning, model training, model deployment, to model monitoring. This is just like a bootcamp that boosts your experience in machine learning engineering.

The Finetuning Large Language Models course is a course that teaches you how to finetune a language model. It is a very practical course that teaches you how to finetune a language model using HuggingFace's Transformers library. 

## Key Takeaways

Some key notebooks during the course (runnable on AWS SageMaker):

1. [Lab 1: summarize dialogue](./Notebooks/Lab_1_summarize_dialogue.ipynb)
2. [Lab 2: fine tune generative model](./Notebooks/Lab_2_fine_tune_generative_ai_model.ipynb)
3. [Lab 3: fine tune LLMs](./Notebooks/Lab_3_fine_tune_model_to_detoxify_summaries.ipynb)

## Conclusion

This course is practical and helpful for entry-level machine learning engineers. 
